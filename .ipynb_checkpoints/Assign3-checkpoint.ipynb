{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2367d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Device Available\n",
      "Name of the Cuda Device:  NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "GPU Computational Capablity:  (8, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([3978])) that is different to the input size (torch.Size([666, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/1000............. Loss: 86.8059\n",
      "Epoch: 20/1000............. Loss: 61.0174\n",
      "Epoch: 30/1000............. Loss: 50.6383\n",
      "Epoch: 40/1000............. Loss: 49.4865\n",
      "Epoch: 50/1000............. Loss: 49.8544\n",
      "Epoch: 60/1000............. Loss: 49.6187\n",
      "Epoch: 70/1000............. Loss: 49.4260\n",
      "Epoch: 80/1000............. Loss: 49.4217\n",
      "Epoch: 90/1000............. Loss: 49.4244\n",
      "Epoch: 100/1000............. Loss: 49.4154\n",
      "Epoch: 110/1000............. Loss: 49.4144\n",
      "Epoch: 120/1000............. Loss: 49.4146\n",
      "Epoch: 130/1000............. Loss: 49.4142\n",
      "Epoch: 140/1000............. Loss: 49.4142\n",
      "Epoch: 150/1000............. Loss: 49.4142\n",
      "Epoch: 160/1000............. Loss: 49.4142\n",
      "Epoch: 170/1000............. Loss: 49.4142\n",
      "Epoch: 180/1000............. Loss: 49.4142\n",
      "Epoch: 190/1000............. Loss: 49.4142\n",
      "Epoch: 200/1000............. Loss: 49.4142\n",
      "Epoch: 210/1000............. Loss: 49.4142\n",
      "Epoch: 220/1000............. Loss: 49.4142\n",
      "Epoch: 230/1000............. Loss: 49.4142\n",
      "Epoch: 240/1000............. Loss: 49.4142\n",
      "Epoch: 250/1000............. Loss: 49.4142\n",
      "Epoch: 260/1000............. Loss: 49.4142\n",
      "Epoch: 270/1000............. Loss: 49.4142\n",
      "Epoch: 280/1000............. Loss: 49.4142\n",
      "Epoch: 290/1000............. Loss: 49.4142\n",
      "Epoch: 300/1000............. Loss: 49.4142\n",
      "Epoch: 310/1000............. Loss: 49.4142\n",
      "Epoch: 320/1000............. Loss: 49.4142\n",
      "Epoch: 330/1000............. Loss: 49.4142\n",
      "Epoch: 340/1000............. Loss: 49.4142\n",
      "Epoch: 350/1000............. Loss: 49.4142\n",
      "Epoch: 360/1000............. Loss: 49.4142\n",
      "Epoch: 370/1000............. Loss: 49.4142\n",
      "Epoch: 380/1000............. Loss: 49.4142\n",
      "Epoch: 390/1000............. Loss: 49.4142\n",
      "Epoch: 400/1000............. Loss: 49.4142\n",
      "Epoch: 410/1000............. Loss: 49.4142\n",
      "Epoch: 420/1000............. Loss: 49.4142\n",
      "Epoch: 430/1000............. Loss: 49.4142\n",
      "Epoch: 440/1000............. Loss: 49.4142\n",
      "Epoch: 450/1000............. Loss: 49.4142\n",
      "Epoch: 460/1000............. Loss: 49.4142\n",
      "Epoch: 470/1000............. Loss: 49.4142\n",
      "Epoch: 480/1000............. Loss: 49.4142\n",
      "Epoch: 490/1000............. Loss: 49.4142\n",
      "Epoch: 500/1000............. Loss: 49.4142\n",
      "Epoch: 510/1000............. Loss: 49.4142\n",
      "Epoch: 520/1000............. Loss: 49.4142\n",
      "Epoch: 530/1000............. Loss: 49.4142\n",
      "Epoch: 540/1000............. Loss: 49.4142\n",
      "Epoch: 550/1000............. Loss: 49.4142\n",
      "Epoch: 560/1000............. Loss: 49.4142\n",
      "Epoch: 570/1000............. Loss: 49.4142\n",
      "Epoch: 580/1000............. Loss: 49.4142\n",
      "Epoch: 590/1000............. Loss: 49.4142\n",
      "Epoch: 600/1000............. Loss: 49.4142\n",
      "Epoch: 610/1000............. Loss: 49.4142\n",
      "Epoch: 620/1000............. Loss: 49.4142\n",
      "Epoch: 630/1000............. Loss: 49.4142\n",
      "Epoch: 640/1000............. Loss: 49.4142\n",
      "Epoch: 650/1000............. Loss: 49.4142\n",
      "Epoch: 660/1000............. Loss: 49.4142\n",
      "Epoch: 670/1000............. Loss: 49.4142\n",
      "Epoch: 680/1000............. Loss: 49.4142\n",
      "Epoch: 690/1000............. Loss: 49.4142\n",
      "Epoch: 700/1000............. Loss: 49.4142\n",
      "Epoch: 710/1000............. Loss: 49.4142\n",
      "Epoch: 720/1000............. Loss: 49.4142\n",
      "Epoch: 730/1000............. Loss: 49.4142\n",
      "Epoch: 740/1000............. Loss: 49.4142\n",
      "Epoch: 750/1000............. Loss: 49.4142\n",
      "Epoch: 760/1000............. Loss: 49.4142\n",
      "Epoch: 770/1000............. Loss: 49.4142\n",
      "Epoch: 780/1000............. Loss: 49.4142\n",
      "Epoch: 790/1000............. Loss: 49.4142\n",
      "Epoch: 800/1000............. Loss: 49.4142\n",
      "Epoch: 810/1000............. Loss: 49.4142\n",
      "Epoch: 820/1000............. Loss: 49.4142\n",
      "Epoch: 830/1000............. Loss: 49.4142\n",
      "Epoch: 840/1000............. Loss: 49.4142\n",
      "Epoch: 850/1000............. Loss: 49.4142\n",
      "Epoch: 860/1000............. Loss: 49.4142\n",
      "Epoch: 870/1000............. Loss: 49.4142\n",
      "Epoch: 880/1000............. Loss: 49.4142\n",
      "Epoch: 890/1000............. Loss: 49.4142\n",
      "Epoch: 900/1000............. Loss: 49.4142\n",
      "Epoch: 910/1000............. Loss: 49.4142\n",
      "Epoch: 920/1000............. Loss: 49.4142\n",
      "Epoch: 930/1000............. Loss: 49.4142\n",
      "Epoch: 940/1000............. Loss: 49.4142\n",
      "Epoch: 950/1000............. Loss: 49.4142\n",
      "Epoch: 960/1000............. Loss: 49.4142\n",
      "Epoch: 970/1000............. Loss: 49.4142\n",
      "Epoch: 980/1000............. Loss: 49.4142\n",
      "Epoch: 990/1000............. Loss: 49.4142\n",
      "Epoch: 1000/1000............. Loss: 49.4142\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Cuda Device Available\")\n",
    "    print(\"Name of the Cuda Device: \", torch.cuda.get_device_name())\n",
    "    print(\"GPU Computational Capablity: \", torch.cuda.get_device_capability())\n",
    "\n",
    "\"\"\"\n",
    "def ChangeDateShiller(date):\n",
    "    date = date.split()\n",
    "    firstElement = date[0]\n",
    "    firstElement = firstElement.split('-')\n",
    "    firstElement[0], firstElement[1] = firstElement[1], firstElement[0]\n",
    "    firstElement[1], firstElement[2] = firstElement[2], firstElement[1]\n",
    "    if len(firstElement[0]) == 1:\n",
    "        firstElement[0] = \"0\" + firstElement\n",
    "    if len(firstElement[1]) == 1:\n",
    "        firstElement[1] = \"0\" + firstElement\n",
    "    firstElement = \"/\".join(firstElement)\n",
    "    return firstElement\n",
    "\"\"\"\n",
    "\n",
    "#Rough Estimate made Graphically\n",
    "def MultiplyPE(pe):\n",
    "    pe = pe * 25\n",
    "    return pe\n",
    "\n",
    "#Scales PE for 2012 y intercept\n",
    "def ScalePERatio(df):\n",
    "    df['Shiller PE Ratio'] = df['Shiller PE Ratio'].apply(MultiplyPE)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def BuildSandPDataSet():\n",
    "    df = pd.read_csv(\"SandP500Data.csv\", sep = ',')\n",
    "    #df.drop('Volume', inplace = True, axis=1)\n",
    "    df = df.sort_index(axis=0,ascending=False).reset_index()\n",
    "    df.drop('index', inplace = True, axis = 1)\n",
    "    return df\n",
    "\n",
    "def BuildShillerPeDataSet():\n",
    "    df = pd.read_csv(\"ShillerPERatio.csv\", sep = ',', skiprows = [0])\n",
    "    df = df.rename(columns={'DateTime': 'Date'})\n",
    "    #df['Date'] = df['Date'].apply(ChangeDateShiller)\n",
    "    df = df.sort_index(axis=0,ascending=False).reset_index()\n",
    "    df.drop('index', inplace = True, axis = 1)\n",
    "    return df\n",
    "\n",
    "def BuildDataset():\n",
    "    SP = BuildSandPDataSet()\n",
    "    PE = BuildShillerPeDataSet()\n",
    "    df = pd.merge(SP, PE, how=\"inner\", on=[\"Date\"])\n",
    "    df = df[['Date', 'Open', 'High', 'Low', 'Volume', 'Shiller PE Ratio', 'Close']]\n",
    "    return df\n",
    "\n",
    "def MergeByDate(df1, df2):\n",
    "    dfinal = df1.merge(df2, on=\"Date\", how = 'inner')\n",
    "    return dfinal\n",
    "\n",
    "\n",
    "#Looks like the derivatives of each are correlated\n",
    "#Even though maybe the magnitudes are not\n",
    "def GraphPEToClose(SandPData, PEData, logy = True):\n",
    "    if logy == False:\n",
    "        #PEData = ScalePERatio(PEData)\n",
    "        title = \"Graph of S&P and Schiller PE\"\n",
    "    else:\n",
    "         title = \"Graph of S&P and Schiller PE\"\n",
    "        \n",
    "    dfinal = MergeByDate(SandPData, PEData)\n",
    "    dfinal[:].plot(x='Date', y=['Close', 'Shiller PE Ratio'], figsize=(10,5), logy = logy, title = title, grid=True)\n",
    "\n",
    "def PEGraph():\n",
    "    SP = BuildSandPDataSet()\n",
    "    PE = BuildShillerPeDataSet()\n",
    "    GraphPEToClose(SP, PE)\n",
    "    \n",
    "def BuildInputSequence(dataset, window, prediction):\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, len(dataset) - window + 1 - prediction):\n",
    "        feature = []\n",
    "        label = []\n",
    "        for j in range(0, window):\n",
    "            row = []\n",
    "            row.append(np.log(dataset['Open'][i + j]))\n",
    "            row.append(np.log(dataset['High'][i + j]))\n",
    "            row.append(np.log(dataset['Low'][i + j]))\n",
    "            row.append(np.log(dataset['Volume'][i + j]))\n",
    "            row.append(np.log(dataset['Shiller PE Ratio'][i + j]))\n",
    "            row.append(np.log(dataset['Close'][i + j]))\n",
    "            feature.append(row)\n",
    "\n",
    "        for j in range(0, prediction):\n",
    "            label.append(np.log(dataset['Close'])[i + window + j])\n",
    "\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return [features, labels]\n",
    "\n",
    "def TrainTestDataset(window = 1, prediction = 1):\n",
    "    df = BuildDataset()\n",
    "    a = BuildInputSequence(df, window, prediction)\n",
    "\n",
    "    X_train = a[0][:round(len(a[0])/2)]\n",
    "    Y_train = a[0][(len(a[0]) - round(len(a[0])/2)) + 1:]\n",
    "\n",
    "    X_test = a[1][:round(len(a[1])/2)]\n",
    "    Y_test = a[1][(len(a[1]) - round(len(a[1])/2)) + 1:]\n",
    "    \n",
    "    return [X_train, Y_train, X_test, Y_test]\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size, layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "    \n",
    "def Train(Model, X_train, Y_train, epochs, learningRate):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(Model.parameters(), lr=learningRate)\n",
    "    \n",
    "    X_train = X_train.to(torch.float32)\n",
    "    X_train = X_train.to(device)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        output, hidden = Model(X_train)\n",
    "        \n",
    "        hidden = hidden.to(torch.float32)\n",
    "        \n",
    "        output = output.to(torch.float32)\n",
    "        output = output.to(device)\n",
    "        \n",
    "        Y_train = Y_train.to(torch.float32)\n",
    "        Y_train = Y_train.to(device)\n",
    "        \n",
    "        loss = criterion(output, Y_train.view(-1).float())\n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "    \n",
    "    \n",
    "#rnn = RNN(6, 32, 1)\n",
    "D = TrainTestDataset(3)\n",
    "X_train, Y_train, X_test, Y_test = D[0], D[1], D[2], D[3]\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "\n",
    "Model = RNN(5, 1, 32, 1, 3)\n",
    "Model = Model.to(device)\n",
    "\n",
    "Train(Model, X_train, Y_train, 1000, 0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de9e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b9357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0b0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
